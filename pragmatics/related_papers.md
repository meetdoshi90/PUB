<details close>
<summary>LREC 22:  A Pragmatics-Centered Evaluation Framework for Natural Language Understanding
PragmEval</summary>
<pre>
PDTB  Penn Discourse Tree Bank
STAC strategic chat conversations manually annotated with negotiation-related information, dialogue acts and discourse structures in the framework of Segmented Discourse Representation Theory
GUM it includes discourse structure annotations according
to Rhetorical Structure Theory
Emergent is composed of pairs of assertions and titles of news articles that are against, for, or neutral with respect to the opinion of the assertion.
SwitchBoard textual transcriptions of dialogues about various topics with annotated speech acts.
MRDA contains textual transcriptions of multi-party real meetings, with speech act annotations.
Persuasion collection of arguments from student essays annotated with factors of persuasiveness with respect to a claim; considered factors are the following: Specificity, Eloquence, Relevance, and Strength.
SarcasmV2 consists of messages from online forums with responses that may or may not be sarcastic according to human annotations
Squinky dataset gathers annotations on Formality, Informativeness, and Implicature, where sentences were graded on a scale from 1 to 7. The Implicature score is defined as the amount of information that is not explicitly expressed in a sentence.
Verifiability collection of online user comments annotated as Verifiable-Experiential (verifiable and about writer’s experience), Verifiable-Non-Experiential, or Unverifiable.
EmoBank aggregates emotion annotations on texts from various domains using the VAD representation format.

</pre>
</details>

<details close>
<summary>ACL 2023: A fine-grained comparison of pragmatic language understanding in humans and language models</summary>
<pre>
ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans to solve the tasks.
Our test materials are a set of English multiple-choice questions curated by expert researchers (Floyd et al., In prep), covering seven diverse pragmatic phenomena.
use zero-shot prompting to evaluate models with varying sizes and training objectives: GPT-2 (Radford et al., 2019), Tk-Instruct (Wang et al., 2022), Flan-T5 (Chung et al., 2022), and InstructGPT (Ouyang et al., 2022).

models perform poorly on humor, irony, and conversational maxims, suggesting difficulty with social conventions and expectations.
For most of the tasks, the question has three parts: a short story context (1-3 sentences), an utterance by one of the characters, and a question about what the character intended to convey
Tasks: Deceit, Indirect speech, Irony, Maxims, Metaphor, Humor, Coherence

</pre>
</details>


<details close>
<summary>ACL 2023 Findings PragmatiCQA: A Dataset for Pragmatic Question Answering in Conversations
</summary>
<pre>
we present PRAGMATICQA, the first large-scale open-domain question answering (QA) dataset featuring 6873 QA pairs that explores pragmatic reasoning in conversations over a diverse set of topics

PRAGMATICQA is collected via crowdsourcing
on English-language material from Fandom.com,
where community-maintained wiki pages are used
as reading materials and basis for answering questions. Therefore, it cannot be guaranteed that the
excerpts from Fandom will be factually correct or
stay unchanged over time, and in turn the answers
in PRAGMATICQA are also not factually verified.

</pre>
</details>


<details close>
<summary>ACL 2023 Findings Contrastive Learning of Sociopragmatic Meaning in Social Media
</summary>
<pre>
collect 16 English witter datasets representing eight different SM tasks to
evaluate our models, including (1) crisis awareness task (Olteanu et al., 2014), (2) emotion
recognition (Mohammad et al., 2018), (3) hateful and offensive language detection (Waseem and
Hovy, 2016; Davidson et al., 2017; Basile et al.,
2019; Zampieri et al., 2019a), (4) humor identification (Meaney et al., 2021), (5) irony and sarcasm detection (Hee et al., 2018; Riloff et al., 2013; Ptácek
et al., 2014; Rajadesingan et al., 2015; Bamman
and Smith, 2015), (6) irony type identification (Hee
et al., 2018) (7) sentiment analysis (Thelwall et al.,
2012; Rosenthal et al., 2017), and (8) stance detection (Mohammad et al., 2016). 

. For emotion recognition task, we utilize (1) PsychExp (Wallbott and
Scherer, 1986), a seven-way classification dataset
of self-described emotional experiences created by
psychologists, and (2) GoEmotion (Demszky et al.,
2020), a dataset of Reddit posts annotated with 27
emotions (we exclude neutral samples). For sarcasm detection task, we use two datasets from the
Internet Argument Corpora (Walker et al., 2012;
Oraby et al., 2016) that posts from debate forums.
For sentiment analysis, we utilize (1) five-class
and binary classification versions of the Stanford
Sentiment Treebank (Socher et al., 2013) (SST-5
and SST-2) that include annotated movie reviews
with sentiment tags, (2) movie review (MR) for binary sentiment classification (Pang and Lee, 2005),
and (3) SentiStrength for YouTube comments (SSYouTube) (Thelwall et al., 2012).

</pre>
</details>


<details close>
<summary>ICLR 2023 Reject: Large language models are not zero-shot communicators</summary>
<pre>
https://openreview.net/forum?id=WgbcOQMNXB
</pre>
</details>

<details close>
<summary>EMNLP 2020: “I’d rather just go to bed”: Understanding Indirect Answers</summary>
<pre>

</pre>
</details>



<details close>
<summary>The Cancellability Test for Conversational
Implicatures</summary>
<pre>

</pre>
</details>


<details close>
<summary></summary>
<pre>

</pre>
</details>


<details close>
<summary></summary>
<pre>

</pre>
</details>


<details close>
<summary></summary>
<pre>

</pre>
</details>


<details close>
<summary></summary>
<pre>

</pre>
</details>


<details close>
<summary></summary>
<pre>

</pre>
</details>


<details close>
<summary></summary>
<pre>

</pre>
</details>


<details close>
<summary></summary>
<pre>

</pre>
</details>


<details close>
<summary></summary>
<pre>

</pre>
</details>


<details close>
<summary></summary>
<pre>

</pre>
</details>


<details close>
<summary></summary>
<pre>

</pre>
</details>


<details close>
<summary></summary>
<pre>

</pre>
</details>


<details close>
<summary></summary>
<pre>

</pre>
</details>


<details close>
<summary></summary>
<pre>

</pre>
</details>


<details close>
<summary></summary>
<pre>

</pre>
</details>


<details close>
<summary></summary>
<pre>

</pre>
</details>


<details close>
<summary></summary>
<pre>

</pre>
</details>
